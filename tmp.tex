\subsection{Diffrent Methods For Missing Values}
Depending on whether using former predictions to impute missing values, we can roughly divide current methods for time series prediction into two categories, imputation based one and non-imputation based one. When faced with missing observations, simply imputing them based on historical data is quite a natural strategy and many methods who originally have predictipn steps\(eg. OGD, Kalman\), in this way, can be easily adjusted to become missing value tolerant. When we perform imputing, we assume that the model can produce meaningful predictions and the former non-ideal prediction won't necessarily result in bad performance eventually.

On the other hand, some researchers think that more reliable mechanisms should be proposed for dealing with missing values. \cite{W1981Estimation} makes assumption of the machanism which generates the missing observations; \cite{Ding2010Time} considers a scarce pattern of missing observations of the observed signal, which in fact makes the sampling frequnce lower and assume that $X_{qt}, X_{2qt}, X_{3qt}...$ are certainly not missing observations; \cite{Anava2015Online} extends both weight vector and the observation vector from $p$ dimensions to $2^d$ dimensions, where $p<d$, to cover all possible stuctures of missing data. Although they all claim to be ideal and the probability of misconvergence do raise with the ratio of missing values for impution based methods, the different restrictions on data and the unsatisfactory time complexity make them flashy. It is hard to design experiments to compare those methods.

Instead, we adjust AERR \cite{Hazan2012Linear} as an online time series prediction methods without imputing missing observations. AERR was originally proposed for linear regression with incomplete training samples. After initialize a weight \boldmath $w_0$, we generate a randomized estimator of the gradient with respect to the missing probability at each iteration $t$. Then we use it to update weights conforming an additive rule that limits the weight to the restriction of Ridge approach. There are two independent sampling steps. Firstly, we sample from past $p$ observations uniformly for $k$ times to get an unbiased estimation of the their expectation. Secondly, we choose from the past $p$ observations for another time and in this one each probability of selecting an observation is associated with the current \boldmath $\frac{(w_{windex}[j])^2}{||w_{windex}||^2}$. Then we multiple this two estimations to get an unbiased estimation of the gradient. A complete explanation can be found in \cite{Hazan2012Linear}. If we unfortunately get a missing value during the sampling process , we replace it with zero or sample again. 

As described above, with slight adjustment for our intended case, we can add AERR based method to our experiments with hope to see how well the strategy of sampling may preform in online time series prediction with missing values.
